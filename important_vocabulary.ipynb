{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from libs.utils import *\n",
    "from libs.tf_idf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:03 root INFO: Đang đọc dữ liệu từ file E:\\Learn Machine Learning\\Project\\Opinion Classifier\\res\\dataset\\nremove_stopwords.xlsx ...\n",
      "\t\tat Line 330 [read_dataset() in utils.py, utils]\n",
      "15:21:04 root INFO: Đọc thành công! \n",
      "\t\tat Line 338 [read_dataset() in utils.py, utils]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repair_words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['tiếng', 'anh', 'thi', 'đầu_vào', 'là', 'khôn...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['đã', 'từng', 'là', 'niềm', 'mơ_ước', 'khi', ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['học_phí', 'hơi', 'cao']</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['̀', '̀', '̣', '̀', '̀', '̉', '̣', '̂', 'đ', ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['thiết_nghĩ', 'đề', 'thi', 'utc', 'nên', 'đổi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        repair_words    labels\n",
       "0  ['tiếng', 'anh', 'thi', 'đầu_vào', 'là', 'khôn...   Neutral\n",
       "1  ['đã', 'từng', 'là', 'niềm', 'mơ_ước', 'khi', ...  Negative\n",
       "2                          ['học_phí', 'hơi', 'cao']  Negative\n",
       "3  ['̀', '̀', '̣', '̀', '̀', '̉', '̣', '̂', 'đ', ...   Neutral\n",
       "4  ['thiết_nghĩ', 'đề', 'thi', 'utc', 'nên', 'đổi...  Positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = r\"E:\\Learn Machine Learning\\Project\\Opinion Classifier\\res\\dataset\\nremove_stopwords.xlsx\"\n",
    "\n",
    "data : pd.DataFrame = read_dataset(DATASET_PATH, \"xlsx\", header=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive', 'Neutral', 'Negative']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels : list[str] = data[\"labels\"].unique().tolist()\n",
    "labels.sort(reverse=True)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "string_pattele = string.punctuation + '\\n'\n",
    "string_pattele\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_punctuation(text, except_char: str = None):\n",
    "    try:\n",
    "        result_text = \"\".join(\n",
    "            char for char in text if char not in string_pattele or (except_char and char == except_char))\n",
    "        result_text = remove_emoji(result_text)\n",
    "    except TypeError:\n",
    "        result_text = None\n",
    "    return result_text\n",
    "\n",
    "def filter_none_text_id(documents : pd.Series | list[str]) -> list[int]:\n",
    "    need_filter : list[str] = [text for text in documents]\n",
    "    none_text_index : list[int] = []\n",
    "    for i, text in enumerate(need_filter):\n",
    "        if (text is None):\n",
    "            print(str(i) + \": \" + str(text))\n",
    "            none_text_index.append(i)\n",
    "    \n",
    "    return none_text_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tokens\"] = data[\"repair_words\"].apply(lambda text: [remove_punctuation(x, '_').split()[0] for x in text.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: list[list[str]] = [doc for doc in data[\"Tokens\"]]\n",
    "labels_corpus: list[str] = data[\"labels\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = features_extraction(corpus)\n",
    "vocabulary = list(vocabulary)\n",
    "vocabulary.sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Xét n-gram = 1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo phân phối tần suất:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    positive_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Positive\"].values:\n",
    "    for token in sen:\n",
    "        positive_frequency[token] += 1\n",
    "\n",
    "negative_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    negative_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Negative\"].values:\n",
    "    for token in sen:\n",
    "        negative_frequency[token] += 1\n",
    "\n",
    "neutral_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    neutral_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Neutral\"].values:\n",
    "    for token in sen:\n",
    "        neutral_frequency[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_frequency = dict(sorted(positive_frequency.items(), key=lambda x:x[1], reverse=True))\n",
    "negative_frequency = dict(sorted(negative_frequency.items(), key=lambda x:x[1], reverse=True))\n",
    "neutral_frequency = dict(sorted(neutral_frequency.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xét độ tạp chất của từng từ trong từ điển:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_score(n_class_samples : list[int]) -> float:\n",
    "    gini : float = 0\n",
    "    n_samples : int = 0\n",
    "    for n in n_class_samples:\n",
    "        n_samples += n\n",
    "    for n in n_class_samples:\n",
    "        gini += (n / n_samples) ** 2\n",
    "    \n",
    "    return 1 - gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_score_vocabulary : dict[str, float] = {}\n",
    "for token in vocabulary:\n",
    "    gini_score_vocabulary[token] = gini_score([\n",
    "        positive_frequency[token],\n",
    "        negative_frequency[token],\n",
    "        neutral_frequency[token]\n",
    "    ])\n",
    "\n",
    "gini_score_vocabulary = dict(sorted(gini_score_vocabulary.items(), key=lambda x:x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "MIN_SAMPLES = 3\n",
    "\n",
    "features_vocabulary : dict[str, str] = {} \n",
    "\n",
    "for key in gini_score_vocabulary:\n",
    "    total_samples = positive_frequency[key] + negative_frequency[key] + neutral_frequency[key]\n",
    "    if gini_score_vocabulary.get(key) <= THRESHOLD and total_samples >= MIN_SAMPLES:\n",
    "        feq_max = max([\n",
    "            positive_frequency[key],\n",
    "            negative_frequency[key],\n",
    "            neutral_frequency[key],\n",
    "        ])\n",
    "        if feq_max == positive_frequency[key]:\n",
    "            features_vocabulary[key] = \"Positive\"\n",
    "        elif feq_max == negative_frequency[key]:\n",
    "            features_vocabulary[key] = \"Negative\"\n",
    "        else:\n",
    "            features_vocabulary[key] = \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcs, bán_nước, bẩn, bốt, bớt, bức_xúc, chiếm, cmn, cư_xử, cất, cắt, cốp, deadline, dong, dí, giang_hồ, giáo_vụ, grab, gà, gậy, hình_như, khâu, lão, lùa, lũ, may_ra, ngoại_hình, ngón, nhan_sắc, nón, nọ, phản_ánh, quát, soát, sập, thiếu_thốn, trườg, tệ, tổn_thương, web, xước, áo_mưa, ó, đơn_phương, đực, ốm, 1k, dắt, chửi, chặn, im, kêu, tắc, đếm, 2k, già, gọn, kẹt, phi, phụ, yếu, đổ, nhà_xe, 100k, 60, 9tr, chật, chốn, gắt, nhân_phẩm, thừa, vô_duyên, đắt, bà, vụ, 20k, 40, bách_khoa, chương, chịu_đựng, chở, cấm, dừng, hậu, học_kỳ, hổ, khó_chịu, làm_ăn, phóng, sửa, thi_công, tụi, xe_đạp, giải_quyết, leo, kém, cháu, a7, chú, mệt, xe, 54, 70, 90, a6, bố_mẹ, bộ_phận, canh, cs1, cuốn, cán_sự, cổ, cửa, dương, fes, gom, huỷ, hài_lòng, hành_chính, khổ, kéo, liên_thông, lì_xì, lườm, lịch_sự, lỗi, mơ_ước, mạnh_dạn, nuốt, nếu_như, oi, rbc, rác, rưỡi, suy, sáng_rực, sóng, sư_phạm, tay_lái, thậm_chí, thế_thì, trách, trốn, trộm, tát, tường, tỏ, từ_bỏ, tự_dưng, u, vch, vàng, vắng, vỏ, vỡ, vững, ám_ảnh, điên, đuổi, định, nặng, trả, thái_độ, cay, chúng_nó, cụ, học_hành, khuân, khách, la, nát, nắng, đèn, bác, bãi, tý, ý_thức, giữ, buồn_cười, béo, chiến_lược, giường, gây, hiến, làm_sao, mùi, ngu, nợ, phút, quyền_lực, rớt, xog, đéo, mất, hề, xe_máy, chán, quay, chấp_nhận, con_người, xong, kia, mũ, cầm, vé, bắt, m, thu, đưa, combo, hạn_chế, kính, mồm, não, thang_máy, thg, tư_duy, xảy, đ, khóc, suy_nghĩ, bảo_vệ, lạ, chả, mạng, t, lượt, nghìn, phí, thông_báo, tập_thể, viettel, sai, à, xuống, mẹ, thân, ý_kiến, cổng, sợ, thiếu, ông, chỗ, toà, ô, tiền, gửi, ôi, mặt, bước, mày, thực_sự, chịu, phía, kiểu, nó, đề, bánh, chai, chia, ghê, gương, hôm, mời, thể_hiện, thực_ra, triệu, tạch, vcl, áp_lực, chết, giấy_tờ, lạc, mác, trở_lại, đánh, họ, mấy, dài, khiến, vãi, đợi, đấy, 3k, dựng, hoc, khô_khan, rượu, tai, triết, đông, trưa, vl, chứ, bị, cái, nhanh, bảo, lâu, khó, cu, cũ, máy, nghành, ngờ, nhân_viên, tan, ảnh_hưởng, mắt, sớm, xếp, dùng, bỏ, dù, nói, đâu, gì, kinh, trượt, bố, cơ_mà, vài, đòi, do, thử, đừng, ra, vào, mà, a8, giảm, xưa, tầng, toàn, vấn_đề, còn, vô, đi, ngồi, "
     ]
    }
   ],
   "source": [
    "for key in features_vocabulary:\n",
    "    if features_vocabulary.get(key) == \"Negative\":\n",
    "        print(key, end=\", \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Xét n-gram = 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    positive_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Positive\"].values:\n",
    "    for token in sen:\n",
    "        positive_frequency[token] += 1\n",
    "\n",
    "negative_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    negative_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Negative\"].values:\n",
    "    for token in sen:\n",
    "        negative_frequency[token] += 1\n",
    "\n",
    "neutral_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    neutral_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Neutral\"].values:\n",
    "    for token in sen:\n",
    "        neutral_frequency[token] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

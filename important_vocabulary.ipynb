{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from libs.utils import *\n",
    "from libs.tf_idf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:21:03 root INFO: Đang đọc dữ liệu từ file E:\\Learn Machine Learning\\Project\\Opinion Classifier\\res\\dataset\\nremove_stopwords.xlsx ...\n",
      "\t\tat Line 330 [read_dataset() in utils.py, utils]\n",
      "15:21:04 root INFO: Đọc thành công! \n",
      "\t\tat Line 338 [read_dataset() in utils.py, utils]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repair_words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['tiếng', 'anh', 'thi', 'đầu_vào', 'là', 'khôn...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['đã', 'từng', 'là', 'niềm', 'mơ_ước', 'khi', ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['học_phí', 'hơi', 'cao']</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['̀', '̀', '̣', '̀', '̀', '̉', '̣', '̂', 'đ', ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['thiết_nghĩ', 'đề', 'thi', 'utc', 'nên', 'đổi...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        repair_words    labels\n",
       "0  ['tiếng', 'anh', 'thi', 'đầu_vào', 'là', 'khôn...   Neutral\n",
       "1  ['đã', 'từng', 'là', 'niềm', 'mơ_ước', 'khi', ...  Negative\n",
       "2                          ['học_phí', 'hơi', 'cao']  Negative\n",
       "3  ['̀', '̀', '̣', '̀', '̀', '̉', '̣', '̂', 'đ', ...   Neutral\n",
       "4  ['thiết_nghĩ', 'đề', 'thi', 'utc', 'nên', 'đổi...  Positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = r\"E:\\Learn Machine Learning\\Project\\Opinion Classifier\\res\\dataset\\nremove_stopwords.xlsx\"\n",
    "\n",
    "data : pd.DataFrame = read_dataset(DATASET_PATH, \"xlsx\", header=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Positive', 'Neutral', 'Negative']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels : list[str] = data[\"labels\"].unique().tolist()\n",
    "labels.sort(reverse=True)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "string_pattele = string.punctuation + '\\n'\n",
    "string_pattele\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_punctuation(text, except_char: str = None):\n",
    "    try:\n",
    "        result_text = \"\".join(\n",
    "            char for char in text if char not in string_pattele or (except_char and char == except_char))\n",
    "        result_text = remove_emoji(result_text)\n",
    "    except TypeError:\n",
    "        result_text = None\n",
    "    return result_text\n",
    "\n",
    "def filter_none_text_id(documents : pd.Series | list[str]) -> list[int]:\n",
    "    need_filter : list[str] = [text for text in documents]\n",
    "    none_text_index : list[int] = []\n",
    "    for i, text in enumerate(need_filter):\n",
    "        if (text is None):\n",
    "            print(str(i) + \": \" + str(text))\n",
    "            none_text_index.append(i)\n",
    "    \n",
    "    return none_text_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tokens\"] = data[\"repair_words\"].apply(lambda text: [remove_punctuation(x, '_').split()[0] for x in text.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: list[list[str]] = [doc for doc in data[\"Tokens\"]]\n",
    "labels_corpus: list[str] = data[\"labels\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = features_extraction(corpus)\n",
    "vocabulary = list(vocabulary)\n",
    "vocabulary.sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo phân phối tần suất:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sen:\n\u001b[0;32m     20\u001b[0m         neutral_frequency[token] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 22\u001b[0m pos_feq_table : pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(positive_frequency, columns\u001b[39m=\u001b[39;49mvocabulary)\n\u001b[0;32m     23\u001b[0m neg_feq_table : pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(negative_frequency)\n\u001b[0;32m     24\u001b[0m neu_feq_table : pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(neutral_frequency)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:448\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    444\u001b[0m missing \u001b[39m=\u001b[39m arrays\u001b[39m.\u001b[39misna()\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# GH10856\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     index \u001b[39m=\u001b[39m _extract_index(arrays[\u001b[39m~\u001b[39;49mmissing])\n\u001b[0;32m    449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[0;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "positive_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    positive_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Positive\"].values:\n",
    "    for token in sen:\n",
    "        positive_frequency[token] += 1\n",
    "\n",
    "negative_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    negative_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Negative\"].values:\n",
    "    for token in sen:\n",
    "        negative_frequency[token] += 1\n",
    "\n",
    "neutral_frequency : dict[str, int] = {}\n",
    "for token in vocabulary:\n",
    "    neutral_frequency[token] = 0\n",
    "for _, label, sen in data[data[\"labels\"] == \"Neutral\"].values:\n",
    "    for token in sen:\n",
    "        neutral_frequency[token] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
